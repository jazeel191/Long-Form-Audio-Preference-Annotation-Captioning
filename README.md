This project demonstrates human audio annotation, preference data generation, and long-form captioning used to support AI and generative audio systems.
The work focuses on deep listening, structured metadata tagging, and high-quality written summaries across multiple music genres.
The project reflects real-world workflows used in audio QA, recommender systems, and creator-focused AI platforms.

Objectives
Perform long-form audio listening without skipping
Generate preference labels based on listening experience
Write clear, grounded listening summaries
Tag accurate metadata for each audio file
Maintain quality assurance and consistency

 Annotation Workflow
1️⃣ Long-Form Listening
Full-length audio tracks (3–8 minutes)
Focus on:
Musical structure
Instrumentation
Mood & emotional tone
Production quality
2️⃣ Preference Annotation
Each audio track is evaluated for:
Overall listener preference (Low / Medium / High)
Emotional tone
Genre consistency
Listening use case (focus, background, creative)
3️⃣ Long-Form Captioning
Detailed written summaries are created describing:
Track progression
Instrument layers
Energy changes
Overall listening experience
These summaries are used for preference datasets and model evaluation.
